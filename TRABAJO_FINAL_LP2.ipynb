{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqCUfEZhh0f"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sidaFVuAggz0",
        "outputId": "04ea0743-d503-45fd-e7f5-298ad4cc0c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2xp4okhlkW",
        "outputId": "5da593e8-3aff-4038-abe7-a5d691eff5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gtts\n",
        "!pip install pandas\n",
        "!pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8q4c720hrDK"
      },
      "source": [
        "\n",
        "# 1. Extraccion de datos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edl65G5Th0_t"
      },
      "source": [
        "## 1.1. APIS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPLJvgeFh2cj"
      },
      "source": [
        "### 1.1.1. REDDIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFxd0p5hzxl",
        "outputId": "0642a6b1-5d50-4f7e-eb3d-f99d3f5b6d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noticias guardadas en el archivo: noticias_reddit.csv\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Configurar Reddit con las credenciales proporcionadas\n",
        "reddit = praw.Reddit(\n",
        "    client_id='QDcdiqBvhWU1SUpEIGdn0A',  # Reemplaza con tu Client ID\n",
        "    client_secret='KYDOkHxil-Z1JBxQaNuiR8ayb7lgcg',  # Reemplaza con tu Client Secret\n",
        "    user_agent='myapp:v1.0 (by /u/Healthy-Present5084)'  # Reemplaza con tu User Agent\n",
        ")\n",
        "\n",
        "# Subreddits relevantes para noticias internacionales y Sudamérica\n",
        "subreddits = ['news', 'worldnews', 'technology', 'science', 'todayilearned', 'peru', 'argentina', 'chile', 'colombia', 'brasil']\n",
        "\n",
        "# Almacenar las publicaciones en una lista\n",
        "posts_data = []\n",
        "\n",
        "# Contador de publicaciones\n",
        "total_posts = 0\n",
        "\n",
        "# Obtener las publicaciones más relevantes de los subreddits (del último mes)\n",
        "for subreddit_name in subreddits:\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Obtener las 20 publicaciones más relevantes del último mes\n",
        "    for submission in subreddit.top(time_filter='month', limit=35):\n",
        "        # Asegurarse de que solo se añadan publicaciones con una puntuación significativa\n",
        "        if submission.score > 10:  # Se puede ajustar el valor mínimo de la puntuación\n",
        "            posts_data.append({\n",
        "                'Título': submission.title,\n",
        "                'Autor': submission.author.name if submission.author else 'N/A',\n",
        "                'Enlace': submission.url,\n",
        "                'Fecha de publicación': datetime.utcfromtimestamp(submission.created_utc).strftime('%d/%m/%Y')\n",
        "            })\n",
        "\n",
        "            total_posts += 1\n",
        "            # Salir si se recopilan 100 publicaciones\n",
        "            if total_posts >= 100:\n",
        "                break\n",
        "\n",
        "    if total_posts >= 100:\n",
        "        break\n",
        "\n",
        "# Crear un DataFrame con los datos recopilados\n",
        "df = pd.DataFrame(posts_data)\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "output_file = 'reddit_noticias.csv'  # Noticias internacionales y de Sudamérica\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "# Mensaje de confirmación\n",
        "print(f\"Noticias guardadas en el archivo: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKisz5ozh_WO"
      },
      "source": [
        "### 1.1.2. NEWS API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYn1JknbiCpT",
        "outputId": "0246ce91-efdb-4de6-ac90-b923bf59bdc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obteniendo noticias...\n",
            "\n",
            "Las noticias se guardaron en 'noticias_news_api.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Tu API Key de News API\n",
        "API_KEY = 'd6b0668d8448484184f52d3898efb6d6'\n",
        "\n",
        "# URL del endpoint de News API para obtener las noticias principales\n",
        "url = 'https://newsapi.org/v2/top-headlines'\n",
        "\n",
        "# Parámetros para la solicitud\n",
        "params = {\n",
        "    'apiKey': API_KEY,  # Tu clave API\n",
        "    'country': 'us',    # País (código de país, puedes cambiarlo por otro como 'es' para España)\n",
        "    'pageSize': 30      # Número de artículos a recuperar\n",
        "}\n",
        "\n",
        "def obtener_noticias():\n",
        "    \"\"\"\n",
        "    Obtiene las últimas noticias.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        news_data = response.json()\n",
        "        return news_data['articles']\n",
        "    else:\n",
        "        print(f\"Error en la solicitud: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def guardar_noticias_csv(noticias):\n",
        "    \"\"\"\n",
        "    Guarda las noticias en un archivo CSV con título, autor, enlace y fecha de publicación.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    authors = []\n",
        "    urls = []\n",
        "    published_dates = []\n",
        "\n",
        "    for article in noticias:\n",
        "        titles.append(article['title'])\n",
        "        authors.append(article['author'] if article['author'] else 'Desconocido')\n",
        "        urls.append(article['url'])\n",
        "\n",
        "        # Formatear la fecha de publicación\n",
        "        if article['publishedAt']:\n",
        "            date_obj = datetime.fromisoformat(article['publishedAt'].replace('Z', ''))\n",
        "            formatted_date = date_obj.strftime('%d/%m/%Y')\n",
        "            published_dates.append(formatted_date)\n",
        "        else:\n",
        "            published_dates.append('No disponible')\n",
        "\n",
        "    # Crear un DataFrame con los datos\n",
        "    df = pd.DataFrame({\n",
        "        'Título': titles,\n",
        "        'Autor': authors,\n",
        "        'Enlace': urls,\n",
        "        'Fecha de publicación': published_dates\n",
        "    })\n",
        "\n",
        "    # Guardar en un archivo CSV\n",
        "    df.to_csv('news_api_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Las noticias se guardaron en 'news_api_noticias.csv'.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Función principal.\n",
        "    \"\"\"\n",
        "    print(\"Obteniendo noticias...\\n\")\n",
        "\n",
        "    # Obtener las noticias\n",
        "    noticias = obtener_noticias()\n",
        "\n",
        "    if noticias:\n",
        "        # Guardar en CSV\n",
        "        guardar_noticias_csv(noticias)\n",
        "    else:\n",
        "        print(\"No se encontraron noticias.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1.3. Youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# API Key de YouTube\n",
        "API_KEY = 'AIzaSyDqElDsdoJ0IjkRhTJgUM6GaOaQwdXPOrg'\n",
        "\n",
        "# URL base de la API de YouTube\n",
        "SEARCH_URL = 'https://www.googleapis.com/youtube/v3/search'\n",
        "VIDEOS_URL = 'https://www.googleapis.com/youtube/v3/videos'\n",
        "\n",
        "# Lista para almacenar los datos de los videos\n",
        "videos = []\n",
        "\n",
        "# Función para buscar videos en tendencia en la categoría de noticias\n",
        "def buscar_videos_tendencia():\n",
        "    videos_resultados = []  # Lista para almacenar los videos\n",
        "\n",
        "    # Definir longitudes de los videos: Medium (4-20 minutos) y Long (más de 20 minutos)\n",
        "    longitudes = ['medium', 'long']\n",
        "\n",
        "    # Realizar la búsqueda para cada longitud de video\n",
        "    for longitud_vid in longitudes:\n",
        "        # Regiones de interés para buscar noticias mundiales (América Latina y España)\n",
        "        regiones = ['PE', 'ES', 'MX', 'AR', 'CL', 'US']  # Perú, España, México, Argentina, Chile, Estados Unidos\n",
        "\n",
        "        for region in regiones:\n",
        "            search_params = {\n",
        "                'part': 'snippet,statistics',\n",
        "                'chart': 'mostPopular',  # Videos populares\n",
        "                'videoCategoryId': '25',  # Filtrar por categoría Noticias\n",
        "                'key': API_KEY,\n",
        "                'maxResults': 20,  # Número máximo de resultados\n",
        "                'relevanceLanguage': 'es',  # Filtrar por videos en español\n",
        "                'regionCode': region,  # Filtrar por videos populares en la región\n",
        "                'videoDuration': longitud_vid,  # Filtrar por duración\n",
        "            }\n",
        "\n",
        "            # Realizar la solicitud a la API\n",
        "            response = requests.get(VIDEOS_URL, params=search_params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for item in data['items']:\n",
        "                    title = item['snippet']['title']\n",
        "                    video_id = item['id']\n",
        "                    channel_title = item['snippet']['channelTitle']\n",
        "                    video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "                    published_at = item['snippet']['publishedAt']\n",
        "\n",
        "                    # Convertir la fecha al formato día/mes/año\n",
        "                    try:\n",
        "                        date_obj = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                        formatted_date = date_obj.strftime(\"%d/%m/%Y\")\n",
        "                    except ValueError:\n",
        "                        formatted_date = 'No disponible'\n",
        "\n",
        "                    videos_resultados.append({\n",
        "                        'Título': title,\n",
        "                        'Autor': channel_title,\n",
        "                        'Enlace': video_url,\n",
        "                        'Fecha de publicación': formatted_date\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Error al acceder a la API para la región {region} y duración {longitud_vid}: {response.status_code}\")\n",
        "\n",
        "    return videos_resultados\n",
        "\n",
        "# Obtener videos de Noticias\n",
        "videos = buscar_videos_tendencia()\n",
        "\n",
        "# Guardar los datos en un archivo CSV\n",
        "if videos:\n",
        "    df = pd.DataFrame(videos)\n",
        "    df.to_csv('youtube_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Los datos se han guardado correctamente en 'youtube_noticias.csv'\")\n",
        "else:\n",
        "    print(\"No se encontraron videos de noticias relevantes en las categorías seleccionadas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. Web Scraping:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.1. El Comercio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# URL de las secciones Mundo y Perú de El Comercio\n",
        "URL_MUNDO = 'https://elcomercio.pe/mundo/?ref=ecr'\n",
        "URL_PERU = 'https://elcomercio.pe/peru/'\n",
        "\n",
        "# Encabezado para evitar bloqueos por bot\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Lista para almacenar las noticias\n",
        "noticias = []\n",
        "\n",
        "# Función para convertir la fecha al formato día/mes/año\n",
        "def formatear_fecha(fecha_texto):\n",
        "    try:\n",
        "        # Intentar convertir la fecha en formato adecuado, eliminando la hora si está presente\n",
        "        # Primero se elimina la parte de la hora si existe\n",
        "        fecha_sin_hora = fecha_texto.split('_')[0].strip()\n",
        "        # Intentar convertir la fecha en el formato dd/mm/yyyy\n",
        "        fecha = datetime.strptime(fecha_sin_hora, '%d/%m/%Y')\n",
        "        return fecha.strftime('%d/%m/%Y')\n",
        "    except ValueError:\n",
        "        # Si no se puede convertir, devolver la fecha original\n",
        "        return fecha_texto\n",
        "\n",
        "# Función para realizar el scraping de una sección\n",
        "def obtener_noticias(url, region):\n",
        "    # Realizar la solicitud HTTP\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    # Verificar que la solicitud fue exitosa\n",
        "    if response.status_code == 200:\n",
        "        # Parsear el contenido HTML\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Buscar los contenedores de las noticias\n",
        "        noticias_elements = soup.find_all('div', class_='story-item')\n",
        "\n",
        "        # Extraer título, enlace, autor y fecha de cada noticia\n",
        "        for idx, noticia in enumerate(noticias_elements):\n",
        "            # Obtener título\n",
        "            title = noticia.find('a', class_='story-item__title')\n",
        "            title_text = title.text.strip() if title else 'No disponible'\n",
        "\n",
        "            # Obtener enlace\n",
        "            link = title['href'] if title else 'No disponible'\n",
        "            full_link = f\"https://elcomercio.pe{link}\" if link != 'No disponible' else 'No disponible'\n",
        "\n",
        "            # Obtener autor (ajustado para capturar la estructura correcta del autor)\n",
        "            author_tag = noticia.find('a', class_='story-item__author')\n",
        "            author_text = author_tag.text.strip() if author_tag else 'No disponible'\n",
        "\n",
        "            # Obtener fecha de publicación y formatearla\n",
        "            date = noticia.find('p', class_='story-item__date')\n",
        "            date_text = date.text.strip() if date else 'No disponible'\n",
        "            date_text = formatear_fecha(date_text)\n",
        "\n",
        "            # Agregar la noticia a la lista con la columna \"Región\"\n",
        "            noticias.append({\n",
        "                'Título': title_text,\n",
        "                'Autor': author_text,\n",
        "                'Enlace': full_link,\n",
        "                'Fecha de publicación': date_text\n",
        "            })\n",
        "    else:\n",
        "        print(f\"Error al acceder a la URL: {url}, código de estado: {response.status_code}\")\n",
        "\n",
        "# Obtener noticias de ambas secciones\n",
        "obtener_noticias(URL_MUNDO, 'Mundial')\n",
        "obtener_noticias(URL_PERU, 'Perú')\n",
        "\n",
        "# Verificar si se encontraron noticias\n",
        "if noticias:\n",
        "    # Convertir los datos en un DataFrame\n",
        "    df = pd.DataFrame(noticias)\n",
        "    df.to_csv('elcomercio_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Las noticias se han almacenado correctamente en 'elcomercio_noticias.csv'\")\n",
        "else:\n",
        "    print(\"No se encontraron noticias. Verifica los selectores HTML.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
