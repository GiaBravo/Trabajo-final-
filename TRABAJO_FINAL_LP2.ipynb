{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Librerias"
      ],
      "metadata": {
        "id": "RQqCUfEZhh0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sidaFVuAggz0",
        "outputId": "04ea0743-d503-45fd-e7f5-298ad4cc0c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts\n",
        "!pip install pandas\n",
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2xp4okhlkW",
        "outputId": "5da593e8-3aff-4038-abe7-a5d691eff5c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Extraccion de datos:"
      ],
      "metadata": {
        "id": "C8q4c720hrDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 APIS:"
      ],
      "metadata": {
        "id": "edl65G5Th0_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **REDDIT:**"
      ],
      "metadata": {
        "id": "HPLJvgeFh2cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar Reddit con las credenciales proporcionadas\n",
        "reddit = praw.Reddit(\n",
        "    client_id='QDcdiqBvhWU1SUpEIGdn0A',  # Reemplaza con tu Client ID\n",
        "    client_secret='KYDOkHxil-Z1JBxQaNuiR8ayb7lgcg',  # Reemplaza con tu Client Secret\n",
        "    user_agent='myapp:v1.0 (by /u/Healthy-Present5084)'  # Reemplaza con tu User Agent\n",
        ")\n",
        "\n",
        "# Subreddits generales y relevantes para noticias internacionales y Sudamérica\n",
        "subreddits = ['news', 'worldnews', 'technology', 'science', 'todayilearned', 'peru', 'argentina', 'chile', 'colombia', 'brasil']\n",
        "\n",
        "# Palabras clave específicas para Perú y Sudamérica\n",
        "keywords = ['Perú', 'Lima', 'noticias de Perú', 'Argentina', 'Chile', 'Colombia', 'Brasil', 'Sudamérica', 'Latin America']\n",
        "\n",
        "# Almacenar las publicaciones en una lista\n",
        "posts_data = []\n",
        "\n",
        "# Contador de publicaciones\n",
        "total_posts = 0\n",
        "\n",
        "# Buscar en los subreddits\n",
        "for subreddit_name in subreddits:\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Obtener las 20 publicaciones principales por relevancia de cada subreddit\n",
        "    for submission in subreddit.top(limit=20):\n",
        "        posts_data.append({\n",
        "                'Título': submission.title,\n",
        "                'Autor': submission.author.name if submission.author else 'N/A',\n",
        "                'Enlace': submission.url ,\n",
        "                'Fecha de publicación': datetime.utcfromtimestamp(submission.created_utc).strftime('%d/%m/%Y')\n",
        "        })\n",
        "\n",
        "        total_posts += 1\n",
        "        # Salir si se recopilan 100 publicaciones\n",
        "        if total_posts >= 100:\n",
        "            break\n",
        "\n",
        "    # Buscar publicaciones con palabras clave relevantes\n",
        "    for keyword in keywords:\n",
        "        for submission in subreddit.search(keyword, limit=20):\n",
        "            posts_data.append({\n",
        "                'Título': submission.title,\n",
        "                'Autor': submission.author.name if submission.author else 'N/A',\n",
        "                'Enlace': submission.url ,\n",
        "                'Fecha de publicación': datetime.utcfromtimestamp(submission.created_utc).strftime('%d/%m/%Y')\n",
        "            })\n",
        "\n",
        "            total_posts += 1\n",
        "            # Salir si se recopilan 100 publicaciones\n",
        "            if total_posts >= 100:\n",
        "                break\n",
        "        if total_posts >= 100:\n",
        "            break\n",
        "    if total_posts >= 100:\n",
        "        break\n",
        "\n",
        "# Crear un DataFrame con los datos recopilados\n",
        "df = pd.DataFrame(posts_data)\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "output_file = 'noticias_reddit.csv' #Noticias internacionales y de Sudamerica\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "# Mensaje de confirmación\n",
        "print(f\"Noticias guardadas en el archivo: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFxd0p5hzxl",
        "outputId": "0642a6b1-5d50-4f7e-eb3d-f99d3f5b6d8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noticias guardadas en el archivo: noticias_reddit.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NEWS API:**"
      ],
      "metadata": {
        "id": "BKisz5ozh_WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Tu API Key de News API\n",
        "API_KEY = 'd6b0668d8448484184f52d3898efb6d6'\n",
        "\n",
        "# URL del endpoint de News API para obtener las noticias principales\n",
        "url = 'https://newsapi.org/v2/top-headlines'\n",
        "\n",
        "# Parámetros para la solicitud\n",
        "params = {\n",
        "    'apiKey': API_KEY,  # Tu clave API\n",
        "    'country': 'us',    # País (código de país, puedes cambiarlo por otro como 'es' para España)\n",
        "    'pageSize': 30      # Número de artículos a recuperar\n",
        "}\n",
        "\n",
        "def obtener_noticias():\n",
        "    \"\"\"\n",
        "    Obtiene las últimas noticias.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        news_data = response.json()\n",
        "        return news_data['articles']\n",
        "    else:\n",
        "        print(f\"Error en la solicitud: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def guardar_noticias_csv(noticias):\n",
        "    \"\"\"\n",
        "    Guarda las noticias en un archivo CSV con título, autor, enlace y fecha de publicación.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    authors = []\n",
        "    urls = []\n",
        "    published_dates = []\n",
        "\n",
        "    for article in noticias:\n",
        "        titles.append(article['title'])\n",
        "        authors.append(article['author'] if article['author'] else 'Desconocido')\n",
        "        urls.append(article['url'])\n",
        "\n",
        "        # Formatear la fecha de publicación\n",
        "        if article['publishedAt']:\n",
        "            date_obj = datetime.fromisoformat(article['publishedAt'].replace('Z', ''))\n",
        "            formatted_date = date_obj.strftime('%d/%m/%Y')\n",
        "            published_dates.append(formatted_date)\n",
        "        else:\n",
        "            published_dates.append('No disponible')\n",
        "\n",
        "    # Crear un DataFrame con los datos\n",
        "    df = pd.DataFrame({\n",
        "        'Título': titles,\n",
        "        'Autor': authors,\n",
        "        'Enlace': urls,\n",
        "        'Fecha de publicación': published_dates\n",
        "    })\n",
        "\n",
        "    # Guardar en un archivo CSV\n",
        "    df.to_csv('noticias_news_api.csv', index=False, encoding='utf-8')\n",
        "    print(\"Las noticias se guardaron en 'noticias_news_api.csv'.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Función principal.\n",
        "    \"\"\"\n",
        "    print(\"Obteniendo noticias...\\n\")\n",
        "\n",
        "    # Obtener las noticias\n",
        "    noticias = obtener_noticias()\n",
        "\n",
        "    if noticias:\n",
        "        # Guardar en CSV\n",
        "        guardar_noticias_csv(noticias)\n",
        "    else:\n",
        "        print(\"No se encontraron noticias.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYn1JknbiCpT",
        "outputId": "0246ce91-efdb-4de6-ac90-b923bf59bdc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obteniendo noticias...\n",
            "\n",
            "Las noticias se guardaron en 'noticias_news_api.csv'.\n"
          ]
        }
      ]
    }
  ]
}