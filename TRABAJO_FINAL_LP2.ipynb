{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"font-size: 2.5em;\">UNIVERSIDAD NACIONAL AGRARIA LA MOLINA</h1>\n",
        "    <h2 style=\"font-size: 1.5em;\">Departamento Académico de Estadística e Informática</h2>\n",
        "    <img src=\"https://www.lamolina.edu.pe/portada/html/acerca/escudos/download/color/856x973_ESCUDOCOLOR.jpg\" alt=\"Escudo de la Universidad\" width=\"200\" style=\"margin: 20px auto;\"/>\n",
        "</div>\n",
        "\n",
        "<p style=\"text-align: center;\"><b>CURSO:</b> Lenguaje De Programación II</p>\n",
        "<p style=\"text-align: center;\"><b>TEMA:</b> Trabajo colaborativo sobre el uso de web scraping y APIs para el análisis de tendencias de noticias</p>\n",
        "\n",
        "### Integrantes  \n",
        "<table style=\"margin: 20px auto; border-collapse: collapse; text-align: center; width: 60%; font-size: 1.1em;\">\n",
        "    <thead>\n",
        "        <tr style=\"background-color: #f2f2f2;\">\n",
        "            <th style=\"border: 1px solid #ddd; padding: 8px;\">**Integrantes**</th>\n",
        "            <th style=\"border: 1px solid #ddd; padding: 8px;\"> **Código** </th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">Ramirez Gutierrez Catherine Antonia</td>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">20220742</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">Galvez-Durand Bravo, Giannella Teresa</td>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">20210945</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">José Ignacio Gamarra Rivas</td>\n",
        "            <td style=\"border: 1px solid #ddd; padding: 8px;\">20180319</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducción:\n",
        "\n",
        "\n",
        "En el contexto actual de la información digital, las tendencias noticiosas juegan un papel fundamental en la toma de decisiones a nivel mundial, desde política hasta entretenimiento. La recopilación de datos sobre las noticias más relevantes se ha vuelto esencial para el análisis y la predicción de tendencias en plataformas como YouTube, Reddit, y sitios web de noticias importantes como El Comercio. Este informe explora cómo el uso de técnicas de web scraping y APIs permite obtener datos relevantes para monitorear y analizar estas tendencias de manera eficiente.\n",
        "\n",
        "\n",
        "\n",
        "## Objetivos:\n",
        "\n",
        "### Objetivo General:\n",
        "\n",
        "- El objetivo principal de este trabajo es extraer datos sobre las noticias a nivel mundial a través de web scraping y APIs, con un enfoque en obtener las noticias más actuales y populares de diversas fuentes. Esto permite comprender mejor cómo se desarrollan las tendencias informativas y cómo se pueden predecir en base a la información extraída de diferentes plataformas\n",
        "\n",
        "### Objetivos Específicos:\n",
        "\n",
        "1. Implementar técnicas de web scraping y consumo de APIs.\n",
        "2. Obtener y analizar noticias más actuales y populares.\n",
        "3. Comparar fuentes de noticias diversas.\n",
        "4. Garantizar la calidad y relevancia de los datos obtenidos.\n",
        "5. Visualizar los resultados de manera comprensible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metodología\n",
        "En este proyecto, se desarrolló un flujo completo de análisis de datos basado en la extracción, integración y visualización de información proveniente de diversas fuentes. El objetivo principal fue comprender patrones de comportamiento, identificar tendencias y generar información valiosa a partir de datos heterogéneos.\n",
        "\n",
        "La metodología aplicada sigue un enfoque sistemático que garantiza la integridad de los datos y la reproducibilidad del análisis. A continuación, se detalla cada etapa del proceso:\n",
        "\n",
        "### 1. Herramientas y funciones utilizadas\n",
        "Se emplearon herramientas clave de Python, junto con funciones específicas que optimizaron cada etapa del proceso:\n",
        "\n",
        "1. **Requests**: Utilizada para realizar solicitudes HTTP a sitios web y obtener el contenido HTML. Esto fue esencial para la etapa de web scraping.\n",
        "   - Función clave: `get()`, que permite acceder a las páginas web y recuperar su contenido.\n",
        "\n",
        "2. **BeautifulSoup**: Permitió analizar y estructurar el contenido HTML recuperado, facilitando la extracción de elementos específicos como titulares y resúmenes de noticias.\n",
        "   - Función clave: `find_all()`, empleada para localizar etiquetas específicas dentro del HTML, como encabezados o párrafos.\n",
        "\n",
        "3. **Pandas**: Utilizada para la manipulación y estructuración de datos. Permitió consolidar la información extraída en tablas, procesarla y eliminar inconsistencias.\n",
        "   - Función clave: `DataFrame()`, que sirvió para organizar los datos en una estructura tabular flexible y manejable.\n",
        "\n",
        "4. **Matplotlib**: Proporcionó herramientas para crear gráficos y visualizar los resultados de manera clara.\n",
        "   - Función clave: `bar()`, utilizada para representar comparaciones entre diferentes fuentes de datos.\n",
        "\n",
        "5. **PRAW (Python Reddit API Wrapper)**: Facilitó el acceso a publicaciones de Reddit, permitiendo filtrar contenido relevante.\n",
        "   - Función clave: `subreddit().hot()`, que permitió obtener publicaciones destacadas de una comunidad específica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Extracción de datos\n",
        "\n",
        "La etapa de extracción de datos incluyó el uso de interfaces de programación de aplicaciones (APIs) y técnicas de extracción directa desde páginas web (web scraping) para acceder a diferentes fuentes de información.\n",
        "\n",
        "### 2.1. APIs\n",
        "Las APIs utilizadas fueron seleccionadas por su capacidad de proporcionar datos actualizados y estructurados de manera eficiente:\n",
        "- **Reddit API**: Permitió extraer publicaciones relacionadas con temas de interés público, aprovechando la popularidad de la plataforma como foro de discusión global. A través de funciones específicas, se accedió a los títulos y descripciones más relevantes.\n",
        "- **News API**: Ofreció acceso a noticias actuales, abarcando una amplia variedad de fuentes informativas reconocidas. Esto facilitó la recopilación de datos estructurados sobre eventos recientes.\n",
        "- **YouTube API**: Proporcionó datos sobre videos relevantes, incluyendo títulos, descripciones y métricas de interacción, fundamentales para comprender tendencias en plataformas de contenido audiovisual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQqCUfEZhh0f"
      },
      "source": [
        "#### Librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sidaFVuAggz0",
        "outputId": "04ea0743-d503-45fd-e7f5-298ad4cc0c40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2xp4okhlkW",
        "outputId": "5da593e8-3aff-4038-abe7-a5d691eff5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.4\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gtts\n",
        "!pip install pandas\n",
        "!pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPLJvgeFh2cj"
      },
      "source": [
        "#### 2.1.1. Reddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJFxd0p5hzxl",
        "outputId": "0642a6b1-5d50-4f7e-eb3d-f99d3f5b6d8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noticias guardadas en el archivo: noticias_reddit.csv\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Configurar Reddit con las credenciales proporcionadas\n",
        "reddit = praw.Reddit(\n",
        "    client_id='QDcdiqBvhWU1SUpEIGdn0A',  # Reemplaza con tu Client ID\n",
        "    client_secret='KYDOkHxil-Z1JBxQaNuiR8ayb7lgcg',  # Reemplaza con tu Client Secret\n",
        "    user_agent='myapp:v1.0 (by /u/Healthy-Present5084)'  # Reemplaza con tu User Agent\n",
        ")\n",
        "\n",
        "# Subreddits relevantes para noticias internacionales y Sudamérica\n",
        "subreddits = ['news', 'worldnews', 'technology', 'science', 'todayilearned', 'peru', 'argentina', 'chile', 'colombia', 'brasil']\n",
        "\n",
        "# Almacenar las publicaciones en una lista\n",
        "posts_data = []\n",
        "\n",
        "# Contador de publicaciones\n",
        "total_posts = 0\n",
        "\n",
        "# Obtener las publicaciones más relevantes de los subreddits (del último mes)\n",
        "for subreddit_name in subreddits:\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "    # Obtener las 20 publicaciones más relevantes del último mes\n",
        "    for submission in subreddit.top(time_filter='month', limit=35):\n",
        "        # Asegurarse de que solo se añadan publicaciones con una puntuación significativa\n",
        "        if submission.score > 10:  # Se puede ajustar el valor mínimo de la puntuación\n",
        "            posts_data.append({\n",
        "                'Título': submission.title,\n",
        "                'Autor': submission.author.name if submission.author else 'N/A',\n",
        "                'Enlace': submission.url,\n",
        "                'Fecha de publicación': datetime.utcfromtimestamp(submission.created_utc).strftime('%d/%m/%Y')\n",
        "            })\n",
        "\n",
        "            total_posts += 1\n",
        "            # Salir si se recopilan 100 publicaciones\n",
        "            if total_posts >= 100:\n",
        "                break\n",
        "\n",
        "    if total_posts >= 100:\n",
        "        break\n",
        "\n",
        "# Crear un DataFrame con los datos recopilados\n",
        "df = pd.DataFrame(posts_data)\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "output_file = 'reddit_noticias.csv'  # Noticias internacionales y de Sudamérica\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "# Mensaje de confirmación\n",
        "print(f\"Noticias guardadas en el archivo: {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKisz5ozh_WO"
      },
      "source": [
        "#### 2.1.2. News API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYn1JknbiCpT",
        "outputId": "0246ce91-efdb-4de6-ac90-b923bf59bdc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obteniendo noticias...\n",
            "\n",
            "Las noticias se guardaron en 'noticias_news_api.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Tu API Key de News API\n",
        "API_KEY = 'd6b0668d8448484184f52d3898efb6d6'\n",
        "\n",
        "# URL del endpoint de News API para obtener las noticias principales\n",
        "url = 'https://newsapi.org/v2/top-headlines'\n",
        "\n",
        "# Parámetros para la solicitud\n",
        "params = {\n",
        "    'apiKey': API_KEY,  # Tu clave API\n",
        "    'country': 'us',    # País (código de país, puedes cambiarlo por otro como 'es' para España)\n",
        "    'pageSize': 30      # Número de artículos a recuperar\n",
        "}\n",
        "\n",
        "def obtener_noticias():\n",
        "    \"\"\"\n",
        "    Obtiene las últimas noticias.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        news_data = response.json()\n",
        "        return news_data['articles']\n",
        "    else:\n",
        "        print(f\"Error en la solicitud: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def guardar_noticias_csv(noticias):\n",
        "    \"\"\"\n",
        "    Guarda las noticias en un archivo CSV con título, autor, enlace y fecha de publicación.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    authors = []\n",
        "    urls = []\n",
        "    published_dates = []\n",
        "\n",
        "    for article in noticias:\n",
        "        titles.append(article['title'])\n",
        "        authors.append(article['author'] if article['author'] else 'Desconocido')\n",
        "        urls.append(article['url'])\n",
        "\n",
        "        # Formatear la fecha de publicación\n",
        "        if article['publishedAt']:\n",
        "            date_obj = datetime.fromisoformat(article['publishedAt'].replace('Z', ''))\n",
        "            formatted_date = date_obj.strftime('%d/%m/%Y')\n",
        "            published_dates.append(formatted_date)\n",
        "        else:\n",
        "            published_dates.append('No disponible')\n",
        "\n",
        "    # Crear un DataFrame con los datos\n",
        "    df = pd.DataFrame({\n",
        "        'Título': titles,\n",
        "        'Autor': authors,\n",
        "        'Enlace': urls,\n",
        "        'Fecha de publicación': published_dates\n",
        "    })\n",
        "\n",
        "    # Guardar en un archivo CSV\n",
        "    df.to_csv('news_api_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Las noticias se guardaron en 'news_api_noticias.csv'.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Función principal.\n",
        "    \"\"\"\n",
        "    print(\"Obteniendo noticias...\\n\")\n",
        "\n",
        "    # Obtener las noticias\n",
        "    noticias = obtener_noticias()\n",
        "\n",
        "    if noticias:\n",
        "        # Guardar en CSV\n",
        "        guardar_noticias_csv(noticias)\n",
        "    else:\n",
        "        print(\"No se encontraron noticias.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1.3. Youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# API Key de YouTube\n",
        "API_KEY = 'AIzaSyDqElDsdoJ0IjkRhTJgUM6GaOaQwdXPOrg'\n",
        "\n",
        "# URL base de la API de YouTube\n",
        "SEARCH_URL = 'https://www.googleapis.com/youtube/v3/search'\n",
        "VIDEOS_URL = 'https://www.googleapis.com/youtube/v3/videos'\n",
        "\n",
        "# Lista para almacenar los datos de los videos\n",
        "videos = []\n",
        "\n",
        "# Función para buscar videos en tendencia en la categoría de noticias\n",
        "def buscar_videos_tendencia():\n",
        "    videos_resultados = []  # Lista para almacenar los videos\n",
        "\n",
        "    # Definir longitudes de los videos: Medium (4-20 minutos) y Long (más de 20 minutos)\n",
        "    longitudes = ['medium', 'long']\n",
        "\n",
        "    # Realizar la búsqueda para cada longitud de video\n",
        "    for longitud_vid in longitudes:\n",
        "        # Regiones de interés para buscar noticias mundiales (América Latina y España)\n",
        "        regiones = ['PE', 'ES', 'MX', 'AR', 'CL', 'US']  # Perú, España, México, Argentina, Chile, Estados Unidos\n",
        "\n",
        "        for region in regiones:\n",
        "            search_params = {\n",
        "                'part': 'snippet,statistics',\n",
        "                'chart': 'mostPopular',  # Videos populares\n",
        "                'videoCategoryId': '25',  # Filtrar por categoría Noticias\n",
        "                'key': API_KEY,\n",
        "                'maxResults': 20,  # Número máximo de resultados\n",
        "                'relevanceLanguage': 'es',  # Filtrar por videos en español\n",
        "                'regionCode': region,  # Filtrar por videos populares en la región\n",
        "                'videoDuration': longitud_vid,  # Filtrar por duración\n",
        "            }\n",
        "\n",
        "            # Realizar la solicitud a la API\n",
        "            response = requests.get(VIDEOS_URL, params=search_params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for item in data['items']:\n",
        "                    title = item['snippet']['title']\n",
        "                    video_id = item['id']\n",
        "                    channel_title = item['snippet']['channelTitle']\n",
        "                    video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "                    published_at = item['snippet']['publishedAt']\n",
        "\n",
        "                    # Convertir la fecha al formato día/mes/año\n",
        "                    try:\n",
        "                        date_obj = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                        formatted_date = date_obj.strftime(\"%d/%m/%Y\")\n",
        "                    except ValueError:\n",
        "                        formatted_date = 'No disponible'\n",
        "\n",
        "                    videos_resultados.append({\n",
        "                        'Título': title,\n",
        "                        'Autor': channel_title,\n",
        "                        'Enlace': video_url,\n",
        "                        'Fecha de publicación': formatted_date\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Error al acceder a la API para la región {region} y duración {longitud_vid}: {response.status_code}\")\n",
        "\n",
        "    return videos_resultados\n",
        "\n",
        "# Obtener videos de Noticias\n",
        "videos = buscar_videos_tendencia()\n",
        "\n",
        "# Guardar los datos en un archivo CSV\n",
        "if videos:\n",
        "    df = pd.DataFrame(videos)\n",
        "    df.to_csv('youtube_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Los datos se han guardado correctamente en 'youtube_noticias.csv'\")\n",
        "else:\n",
        "    print(\"No se encontraron videos de noticias relevantes en las categorías seleccionadas.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Web Scraping\n",
        "El web scraping complementó la obtención de datos mediante la extracción de información directamente de sitios web relevantes. Por ejemplo, se recopilaron titulares y resúmenes de noticias desde portales como \"El Comercio\", proporcionando un contexto adicional no disponible a través de las APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.1. El Comercio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# URL de las secciones Mundo y Perú de El Comercio\n",
        "URL_MUNDO = 'https://elcomercio.pe/mundo/?ref=ecr'\n",
        "URL_PERU = 'https://elcomercio.pe/peru/'\n",
        "\n",
        "# Encabezado para evitar bloqueos por bot\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Lista para almacenar las noticias\n",
        "noticias = []\n",
        "\n",
        "# Función para convertir la fecha al formato día/mes/año\n",
        "def formatear_fecha(fecha_texto):\n",
        "    try:\n",
        "        # Intentar convertir la fecha en formato adecuado, eliminando la hora si está presente\n",
        "        # Primero se elimina la parte de la hora si existe\n",
        "        fecha_sin_hora = fecha_texto.split('_')[0].strip()\n",
        "        # Intentar convertir la fecha en el formato dd/mm/yyyy\n",
        "        fecha = datetime.strptime(fecha_sin_hora, '%d/%m/%Y')\n",
        "        return fecha.strftime('%d/%m/%Y')\n",
        "    except ValueError:\n",
        "        # Si no se puede convertir, devolver la fecha original\n",
        "        return fecha_texto\n",
        "\n",
        "# Función para realizar el scraping de una sección\n",
        "def obtener_noticias(url, region):\n",
        "    # Realizar la solicitud HTTP\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    # Verificar que la solicitud fue exitosa\n",
        "    if response.status_code == 200:\n",
        "        # Parsear el contenido HTML\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Buscar los contenedores de las noticias\n",
        "        noticias_elements = soup.find_all('div', class_='story-item')\n",
        "\n",
        "        # Extraer título, enlace, autor y fecha de cada noticia\n",
        "        for idx, noticia in enumerate(noticias_elements):\n",
        "            # Obtener título\n",
        "            title = noticia.find('a', class_='story-item__title')\n",
        "            title_text = title.text.strip() if title else 'No disponible'\n",
        "\n",
        "            # Obtener enlace\n",
        "            link = title['href'] if title else 'No disponible'\n",
        "            full_link = f\"https://elcomercio.pe{link}\" if link != 'No disponible' else 'No disponible'\n",
        "\n",
        "            # Obtener autor (ajustado para capturar la estructura correcta del autor)\n",
        "            author_tag = noticia.find('a', class_='story-item__author')\n",
        "            author_text = author_tag.text.strip() if author_tag else 'No disponible'\n",
        "\n",
        "            # Obtener fecha de publicación y formatearla\n",
        "            date = noticia.find('p', class_='story-item__date')\n",
        "            date_text = date.text.strip() if date else 'No disponible'\n",
        "            date_text = formatear_fecha(date_text)\n",
        "\n",
        "            # Agregar la noticia a la lista con la columna \"Región\"\n",
        "            noticias.append({\n",
        "                'Título': title_text,\n",
        "                'Autor': author_text,\n",
        "                'Enlace': full_link,\n",
        "                'Fecha de publicación': date_text\n",
        "            })\n",
        "    else:\n",
        "        print(f\"Error al acceder a la URL: {url}, código de estado: {response.status_code}\")\n",
        "\n",
        "# Obtener noticias de ambas secciones\n",
        "obtener_noticias(URL_MUNDO, 'Mundial')\n",
        "obtener_noticias(URL_PERU, 'Perú')\n",
        "\n",
        "# Verificar si se encontraron noticias\n",
        "if noticias:\n",
        "    # Convertir los datos en un DataFrame\n",
        "    df = pd.DataFrame(noticias)\n",
        "    df.to_csv('elcomercio_noticias.csv', index=False, encoding='utf-8')\n",
        "    print(\"Las noticias se han almacenado correctamente en 'elcomercio_noticias.csv'\")\n",
        "else:\n",
        "    print(\"No se encontraron noticias. Verifica los selectores HTML.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Integración de datos\n",
        "Una vez extraídos los datos, estos fueron integrados en un marco común. Este proceso incluyó la eliminación de duplicados, la normalización de formatos y el manejo de valores faltantes. La integración permitió combinar múltiples fuentes de información en una estructura coherente, facilitando el análisis comparativo y la identificación de patrones comunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer los archivos CSV generados previamente\n",
        "df_youtube = pd.read_csv('youtube_noticias.csv')\n",
        "df_reddit = pd.read_csv('reddit_noticias.csv')\n",
        "df_news_api = pd.read_csv('news_api_noticias.csv')\n",
        "df_elcomercio = pd.read_csv('elcomercio_noticias.csv')\n",
        "\n",
        "# Agregar la columna 'Fuente' para identificar la fuente de cada conjunto de datos\n",
        "df_youtube['Fuente'] = 'YouTube'\n",
        "df_reddit['Fuente'] = 'Reddit'\n",
        "df_news_api['Fuente'] = 'News API'\n",
        "df_elcomercio['Fuente'] = 'El Comercio'\n",
        "\n",
        "# Concatenar todos los DataFrames en uno solo\n",
        "df_final = pd.concat([df_youtube, df_reddit, df_news_api, df_elcomercio], ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame final en un nuevo archivo CSV\n",
        "df_final.to_csv('noticias_unidas.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Los datos han sido unidos y guardados en 'noticias_unidas.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Visualización\n",
        "La visualización de datos fue una etapa crucial para interpretar los resultados del análisis. Se generaron gráficos que destacaron patrones y tendencias clave, como la distribución de datos según las fuentes y la frecuencia de temas abordados. La representación visual facilitó la comunicación de hallazgos de manera clara y accesible para diversos públicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.1. Grafico de tendencias en las noticias Mundiales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Descargar las stopwords de NLTK (si aún no se han descargado)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Leer el archivo CSV con las noticias unidas\n",
        "df = pd.read_csv('noticias_unidas.csv')\n",
        "\n",
        "# Asegurarnos de que estamos trabajando con la columna 'Título'\n",
        "titulos = df['Título'].dropna()  # Eliminar cualquier valor nulo\n",
        "\n",
        "# Tokenización: Convertir los títulos a minúsculas y dividir en palabras\n",
        "palabras = []\n",
        "for titulo in titulos:\n",
        "    palabras.extend(titulo.lower().split())\n",
        "\n",
        "# Eliminar signos de puntuación\n",
        "palabras = [palabra.strip(string.punctuation) for palabra in palabras]\n",
        "\n",
        "# Función para eliminar emojis utilizando expresiones regulares\n",
        "def eliminar_emojis(texto):\n",
        "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', texto)\n",
        "\n",
        "# Aplicar la función para eliminar emojis de cada palabra\n",
        "palabras = [eliminar_emojis(palabra) for palabra in palabras]\n",
        "\n",
        "# Obtener las stopwords en español e inglés\n",
        "stop_words_spanish = set(stopwords.words('spanish'))\n",
        "stop_words_english = set(stopwords.words('english'))\n",
        "\n",
        "# Lista de países y términos a excluir (en minúsculas)\n",
        "paises_a_excluir = ['perú', 'Perú', 'España', 'México', 'argentina', 'chile', 'Estados Unidos', 'colombia', 'brasil', ]\n",
        "otras_palabras_a_excluir = ['noticias', 'año', 'años', 'diciembre', 'lima', 'says', 'news']\n",
        "\n",
        "# Combinar ambas listas de stopwords y las palabras adicionales a excluir\n",
        "stop_words = stop_words_spanish.union(stop_words_english).union(paises_a_excluir).union(otras_palabras_a_excluir)\n",
        "\n",
        "# Filtrar las palabras eliminando las stopwords, las palabras de la lista personalizada, los números y los emojis\n",
        "palabras_filtradas = [palabra for palabra in palabras if palabra not in stop_words and not palabra.isnumeric() and palabra != '']\n",
        "\n",
        "# Contar la frecuencia de las palabras\n",
        "frecuencia_palabras = Counter(palabras_filtradas)\n",
        "\n",
        "# Obtener las 10 palabras más comunes (puedes cambiar el número)\n",
        "top_palabras = frecuencia_palabras.most_common(10)\n",
        "\n",
        "# Crear un DataFrame para las palabras y sus frecuencias\n",
        "df_top_palabras = pd.DataFrame(top_palabras, columns=['Palabra', 'Frecuencia'])\n",
        "\n",
        "# Graficar los resultados en un gráfico de pie\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(df_top_palabras['Frecuencia'], labels=df_top_palabras['Palabra'], autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Frecuencia de las Palabras Más Comunes en los Títulos de Noticias')\n",
        "plt.axis('equal')  # Para que el gráfico de pie sea circular\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. Grafico comparativo de tendencias de noticias en redes sociales (Youtube, Reddit) y diarios (News, El Comercio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Descargar las stopwords de NLTK (si aún no se han descargado)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Función para eliminar emojis utilizando expresiones regulares\n",
        "def eliminar_emojis(texto):\n",
        "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', texto)\n",
        "\n",
        "# Función para preprocesar y contar las palabras\n",
        "def procesar_palabras(df, paises_a_excluir, otras_palabras_a_excluir):\n",
        "    # Asegurarnos de que estamos trabajando con la columna 'Título'\n",
        "    titulos = df['Título'].dropna()  # Eliminar cualquier valor nulo\n",
        "\n",
        "    # Tokenización: Convertir los títulos a minúsculas y dividir en palabras\n",
        "    palabras = []\n",
        "    for titulo in titulos:\n",
        "        palabras.extend(titulo.lower().split())\n",
        "\n",
        "    # Eliminar signos de puntuación\n",
        "    palabras = [palabra.strip(string.punctuation) for palabra in palabras]\n",
        "\n",
        "    # Obtener las stopwords en español e inglés\n",
        "    stop_words_spanish = set(stopwords.words('spanish'))\n",
        "    stop_words_english = set(stopwords.words('english'))\n",
        "\n",
        "    # Combinar ambas listas de stopwords y las palabras adicionales a excluir\n",
        "    stop_words = stop_words_spanish.union(stop_words_english).union(paises_a_excluir).union(otras_palabras_a_excluir)\n",
        "\n",
        "    # Filtrar las palabras eliminando las stopwords, las palabras de la lista personalizada, los números y los emojis\n",
        "    palabras_filtradas = [palabra for palabra in palabras if palabra not in stop_words and not palabra.isnumeric() and palabra != '']\n",
        "\n",
        "    # Aplicar la función para eliminar emojis de cada palabra\n",
        "    palabras_filtradas = [eliminar_emojis(palabra) for palabra in palabras_filtradas]\n",
        "\n",
        "    # Contar la frecuencia de las palabras\n",
        "    frecuencia_palabras = Counter(palabras_filtradas)\n",
        "\n",
        "    return frecuencia_palabras\n",
        "\n",
        "# Leer el archivo CSV de noticias con la columna de 'Fuente'\n",
        "df_noticias = pd.read_csv('noticias_unidas.csv')\n",
        "\n",
        "# Lista de países y términos a excluir (en minúsculas)\n",
        "paises_a_excluir = ['perú', 'argentina', 'chile', 'colombia', 'brasil', 'españa', 'estados unidos', 'mexico', 'uruguay', 'venezuela']\n",
        "otras_palabras_a_excluir = ['noticias','say', 'news', 'cnn','diciembre', 'años', 'año', '¿qué', 'que', 'hoy', 'mundo', 'tras', 'lima', 'says', 'found', 'us', 'talks', \"argentina's\"]\n",
        "\n",
        "# Filtrar noticias de YouTube y Reddit\n",
        "df_youtube_reddit = df_noticias[df_noticias['Fuente'].isin(['Youtube', 'Reddit'])]\n",
        "\n",
        "# Filtrar noticias de News API y El Comercio\n",
        "df_news_elcomercio = df_noticias[df_noticias['Fuente'].isin(['News API', 'El Comercio'])]\n",
        "\n",
        "# Procesar los datos de YouTube/Reddit\n",
        "frecuencia_youtube_reddit = procesar_palabras(df_youtube_reddit, paises_a_excluir, otras_palabras_a_excluir)\n",
        "\n",
        "# Procesar los datos de News API/El Comercio\n",
        "frecuencia_news_elcomercio = procesar_palabras(df_news_elcomercio, paises_a_excluir, otras_palabras_a_excluir)\n",
        "\n",
        "# Obtener las 10 palabras más comunes para cada conjunto de datos\n",
        "top_youtube_reddit = frecuencia_youtube_reddit.most_common(10)\n",
        "top_news_elcomercio = frecuencia_news_elcomercio.most_common(10)\n",
        "\n",
        "# Crear DataFrames para los gráficos\n",
        "df_top_youtube_reddit = pd.DataFrame(top_youtube_reddit, columns=['Palabra', 'Frecuencia'])\n",
        "df_top_news_elcomercio = pd.DataFrame(top_news_elcomercio, columns=['Palabra', 'Frecuencia'])\n",
        "\n",
        "# Gráfico de barras para YouTube/Reddit\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(df_top_youtube_reddit['Palabra'], df_top_youtube_reddit['Frecuencia'], color='lightblue')\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.title('Palabras Más Comunes en Títulos de Noticias (YouTube/Reddit)')\n",
        "plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar la palabra más frecuente en la parte superior\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de barras para News API/El Comercio\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(df_top_news_elcomercio['Palabra'], df_top_news_elcomercio['Frecuencia'], color='lightgreen')\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.title('Palabras Más Comunes en Títulos de Noticias (News API/El Comercio)')\n",
        "plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar la palabra más frecuente en la parte superior\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusiones:\n",
        "- **Eficiencia del método**: La metodología empleada demostró ser efectiva para analizar y procesar datos provenientes de múltiples fuentes, combinando tecnologías avanzadas de extracción y análisis.\n",
        "- **Relevancia de la integración**: La integración de datos fue fundamental para obtener resultados coherentes y significativos, destacando la importancia de una estructura bien organizada en proyectos de análisis de datos.\n",
        "- **Aplicabilidad y escalabilidad**: Este enfoque es altamente versátil y puede aplicarse a proyectos de mayor envergadura, como el monitoreo en tiempo real de tendencias o la creación de sistemas automatizados de generación de reportes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
